AWSTemplateFormatVersion: 2010-09-09
Description: >
  Purple-Demo – EKS cluster with SageMaker ACK controller (atomic Helm),
  cleanup Job, and DVWA-ready node group – single-template version.

Parameters:
  ClusterName:
    Type: String
    Default: purple-demo
  NodeInstanceType:
    Type: String
    Default: t3.medium
  NodeDesiredCapacity:
    Type: Number
    Default: 3
  VpcCidr:
    Type: String
    Default: 10.0.0.0/16
  EksVersion:
    Type: String
    Default: "1.28"
  S3Bucket:
    Type: String
    Default: my-purple-game-assets-307946665489-us-east-2
    Description: S3 bucket containing the Helm handler Lambda code
  S3Key:
    Type: String
    Default: ack-helm-handler.zip
    Description: S3 key for the Helm handler Lambda code

Mappings:
  SubnetConfig:
    PublicA:  { Cidr: "10.0.0.0/24", AZSuffix: "a" }
    PublicB:  { Cidr: "10.0.1.0/24", AZSuffix: "b" }
    PrivateA:  { Cidr: "10.0.10.0/24", AZSuffix: "a" }
    PrivateB:  { Cidr: "10.0.11.0/24", AZSuffix: "b" }

Resources:
# ---------------------------------------------------------------------------
# VPC with two AZs, 1 NAT GW
# ---------------------------------------------------------------------------
  Vpc:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidr
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags: [{ Key: Name, Value: !Sub '${ClusterName}-Vpc' }]

  InternetGateway:
    Type: AWS::EC2::InternetGateway

  AttachIg:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref Vpc
      InternetGatewayId: !Ref InternetGateway

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref Vpc
  PublicRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  # Create four subnets + associations
  PublicSubnetA:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref Vpc
      AvailabilityZone: !Join 
        - ''
        - - !Ref 'AWS::Region'
          - !FindInMap [ SubnetConfig, PublicA, AZSuffix ]
      CidrBlock:  !FindInMap [ SubnetConfig, PublicA, Cidr ]
      MapPublicIpOnLaunch: true
      Tags: [{Key: Name, Value: !Sub '${ClusterName}-PublicA'}]
  PublicSubnetB:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref Vpc
      AvailabilityZone: !Join ['', [ !Ref 'AWS::Region', !FindInMap [ SubnetConfig, PublicB, AZSuffix ] ] ]
      CidrBlock:  !FindInMap [ SubnetConfig, PublicB, Cidr ]
      MapPublicIpOnLaunch: true
      Tags: [{Key: Name, Value: !Sub '${ClusterName}-PublicB'}]
  PrivateSubnetA:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref Vpc
      AvailabilityZone: !Join ['', [ !Ref 'AWS::Region', !FindInMap [ SubnetConfig, PrivateA, AZSuffix ] ] ]
      CidrBlock:  !FindInMap [ SubnetConfig, PrivateA, Cidr ]
      Tags: [{Key: Name, Value: !Sub '${ClusterName}-PrivateA'}]
  PrivateSubnetB:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref Vpc
      AvailabilityZone: !Join ['', [ !Ref 'AWS::Region', !FindInMap [ SubnetConfig, PrivateB, AZSuffix ] ] ]
      CidrBlock:  !FindInMap [ SubnetConfig, PrivateB, Cidr ]
      Tags: [{Key: Name, Value: !Sub '${ClusterName}-PrivateB'}]

  PublicSubnetRouteA:  # associate
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnetA
      RouteTableId: !Ref PublicRouteTable
  PublicSubnetRouteB:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnetB
      RouteTableId: !Ref PublicRouteTable

  NatGatewayEIP:
    Type: AWS::EC2::EIP
    Properties: { Domain: vpc }
  NatGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt NatGatewayEIP.AllocationId
      SubnetId:     !Ref PublicSubnetA

  PrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties: { VpcId: !Ref Vpc }
  PrivateRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NatGateway
  PrivateSubnetRouteA:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnetA
      RouteTableId: !Ref PrivateRouteTable
  PrivateSubnetRouteB:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnetB
      RouteTableId: !Ref PrivateRouteTable

# ---------------------------------------------------------------------------
#  EKS Cluster & NodeGroup
# ---------------------------------------------------------------------------
  EksServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Effect: Allow
          Principal: { Service: [ eks.amazonaws.com ] }
          Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy

  NodeRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Effect: Allow
          Principal:
            Service: [ ec2.amazonaws.com ]
          Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      Tags:
        - Key: Name
          Value: !Sub '${ClusterName}-NodeRole'

  EksCluster:
    Type: AWS::EKS::Cluster
    Properties:
      Name:   !Ref ClusterName
      KubernetesNetworkConfig:
        ServiceIpv4Cidr: 172.20.0.0/16
      Version: !Ref EksVersion
      ResourcesVpcConfig:
        SubnetIds:
          - !Ref PublicSubnetA
          - !Ref PublicSubnetB
          - !Ref PrivateSubnetA
          - !Ref PrivateSubnetB
      RoleArn: !GetAtt EksServiceRole.Arn
      AccessConfig:
        AuthenticationMode: API_AND_CONFIG_MAP

  NodeGroup:
    DependsOn: EksCluster
    Type: AWS::EKS::Nodegroup
    Properties:
      ClusterName: !Ref ClusterName
      NodegroupName: default-ng
      ScalingConfig:
        DesiredSize: !Ref NodeDesiredCapacity
        MaxSize: !Ref NodeDesiredCapacity
        MinSize: !Ref NodeDesiredCapacity
      DiskSize: 20
      InstanceTypes: [ !Ref NodeInstanceType ]
      Subnets:
        - !Ref PrivateSubnetA
        - !Ref PrivateSubnetB
      NodeRole: !GetAtt NodeRole.Arn
      AmiType: AL2_x86_64

  ClusterAdminAccess:
    Type: AWS::EKS::AccessEntry
    DependsOn:
      - EksCluster
      - AckSageMakerFunctionRole
    Properties:
      ClusterName: !Ref ClusterName
      PrincipalArn: !GetAtt AckSageMakerFunctionRole.Arn
      Type: STANDARD
      AccessPolicies:
        - PolicyArn: arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy
          AccessScope:
            Type: cluster

  RootAccountAccess:
    Type: AWS::EKS::AccessEntry
    DependsOn:
      - EksCluster
    Properties:
      ClusterName: !Ref ClusterName
      PrincipalArn: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
      Type: STANDARD
      AccessPolicies:
        - PolicyArn: arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy
          AccessScope:
            Type: cluster

  CfnK8sExecutionRoleAccess:
    Type: AWS::EKS::AccessEntry
    DependsOn:
      - EksCluster
    Properties:
      ClusterName: !Ref ClusterName
      PrincipalArn: !GetAtt CfnK8sExecutionRole.Arn
      Type: STANDARD
      AccessPolicies:
        - PolicyArn: arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy
          AccessScope:
            Type: cluster

# ---------------------------------------------------------------------------
#  Namespace, IRSA & Cleanup Job (AWSQS::Kubernetes::Resource)
#    (AWSQS public extension “Kubernetes::Resource” replaces the older
#     Manifest type.)
# ---------------------------------------------------------------------------

  ApplyManifestFunction:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.12
      Handler: index.handler
      Timeout: 300
      MemorySize: 128
      Role: !GetAtt AckSageMakerFunctionRole.Arn   # reuse existing role with eks:AccessKubernetesApi
      Code:
        ZipFile: |
          import json, boto3, cfnresponse, re
          eks = boto3.client("eks")
          def handler(event, context):
              rp = event['ResourceProperties']
              cluster = rp['Cluster']
              manifest_list = rp['Manifest']          # list of YAML (or JSON) strings
              try:
                  if event['RequestType'] == 'Delete':
                    # Best‑effort delete based on regex (no PyYAML dependency)
                    for doc in manifest_list:
                      # extract kind, name, namespace with simple regex
                      kind_m = re.search(r'kind:\s*(\w+)', doc)
                      name_m = re.search(r'name:\s*([-\w]+)', doc)
                      ns_m   = re.search(r'namespace:\s*([-\w]+)', doc)
                      if not (kind_m and name_m):
                          continue
                      kind = kind_m.group(1).lower() + 's'
                      name = name_m.group(1)
                      ns = ns_m.group(1) if ns_m else None
                      uri = f"/apis/apps/v1/namespaces/{ns}/{kind}/{name}" if ns else f"/api/v1/{kind}/{name}"
                      eks.execute_kubernetes_api(clusterName=cluster, method='DELETE', uri=uri)
                    cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  else:
                      for doc in manifest_list:
                          eks.execute_kubernetes_api(
                              clusterName=cluster,
                              method='POST',
                              uri='/',
                              contentType='application/yaml',
                              body=doc if isinstance(doc, str) else json.dumps(doc)
                          )
              except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})

  AckSystemNamespace:
    Type: Custom::ApplyManifest
    DependsOn: NodeGroup
    Properties:
      ServiceToken: !GetAtt ApplyManifestFunction.Arn
      Cluster: !Ref ClusterName
      Manifest:
        - |
          apiVersion: v1
          kind: Namespace
          metadata:
            name: ack-system

  SageMakerServiceAccount:
    Type: Custom::ApplyManifest
    DependsOn:
      - AckSystemNamespace
    Properties:
      ServiceToken: !GetAtt ApplyManifestFunction.Arn
      Cluster:  !Ref ClusterName
      Manifest:
        - !Sub |
            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: ack-sagemaker-controller
              namespace: ack-system
              annotations:
                eks.amazonaws.com/role-arn: ${SageMakerIrsaRole.Arn}

# ---------------------------------------------------------------------------
#  OIDC provider (discovered from the cluster) and IRSA role for ACK controller
# ---------------------------------------------------------------------------
  OidcProvider:
    Type: AWS::IAM::OIDCProvider
    Properties:
      Url: !GetAtt EksCluster.OpenIdConnectIssuerUrl
      ClientIdList: [ sts.amazonaws.com ]
      ThumbprintList: [ 9e99a48a9960b14926bb7f3b02e22da0cedb8061 ] # Amazon Root CA1

  SageMakerIrsaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Effect: Allow
          Principal: { Federated: !Ref OidcProvider }
          Action: sts:AssumeRoleWithWebIdentity
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess


# ---------------------------------------------------------------------------
#  Lambda‑backed custom resource that installs / removes the SageMaker ACK
#  controller via Helm (replaces deprecated AWSQS::Kubernetes::Helm type)
# ---------------------------------------------------------------------------

  AckSageMakerFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'CFN-Registry-ExecRole-${ClusterName}'
      AssumeRolePolicyDocument:
        Statement:
        - Effect: Allow
          Principal: { Service: [ lambda.amazonaws.com ] }
          Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
      - PolicyName: EksHelmAccess
        PolicyDocument:
          Statement:
          - Effect: Allow
            Action:
              - eks:DescribeCluster
              - eks:AccessKubernetesApi
            Resource: "*"
          - Effect: Allow
            Action: iam:PassRole
            Resource: "*"     # restrict in prod

  # NEW – execution role for AWSQS::Kubernetes::Resource
  CfnK8sExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'CFN-K8s-ExecutionRole-${ClusterName}'
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - resources.cloudformation.amazonaws.com
                - lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: AllowEKSAccess
          PolicyDocument:
            Statement:
              - Effect: Allow
                Action:
                  - ec2:CreateNetworkInterface
                  - ec2:DeleteNetworkInterface
                  - ec2:DescribeSecurityGroups
                  - ec2:DescribeSubnets
                  - ec2:DescribeVpcs
                  - eks:AccessKubernetesApi
                  - eks:DescribeCluster
                  - lambda:CreateFunction
                  - lambda:DeleteFunction
                  - lambda:InvokeFunction
                  - lambda:UpdateFunctionCode
                  - lambda:UpdateFunctionConfiguration
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:PutRetentionPolicy
                  - s3:GetObject
                  - ssm:GetParameter
                  - sts:GetCallerIdentity
                Resource: "*"
              - Effect: Allow
                Action: iam:PassRole
                Resource: "*"

  AckSageMakerFunction:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.12
      Handler: index.handler
      Timeout: 600
      MemorySize: 256
      Role: !GetAtt AckSageMakerFunctionRole.Arn
      Code:
        S3Bucket: !Ref S3Bucket
        S3Key: !Ref S3Key

  AckSageMakerCustom:
    Type: AWS::CloudFormation::CustomResource
    DependsOn: SageMakerServiceAccount   # wait for namespace + SA
    Properties:
      ServiceToken: !GetAtt AckSageMakerFunction.Arn
      ClusterName: !Ref ClusterName
      Namespace:   ack-system
      ChartRepo:   oci://public.ecr.aws/aws-controllers-k8s/sagemaker-chart
      ChartName:   sagemaker-chart
      ChartVersion: "1.2.16"
      Values: !Sub |
        aws:
          region: ${AWS::Region}
        serviceAccount:
          create: false
          name: ack-sagemaker-controller

# ---------------------------------------------------------------------------
#  DVWA demo application (namespace + Deployment & Service)
# ---------------------------------------------------------------------------

  DvwaNamespace:
    Type: Custom::ApplyManifest
    DependsOn: NodeGroup
    Properties:
      ServiceToken: !GetAtt ApplyManifestFunction.Arn
      Cluster: !Ref ClusterName
      Manifest:
        - |
          apiVersion: v1
          kind: Namespace
          metadata:
            name: dvwa

  DvwaApp:
    Type: Custom::ApplyManifest
    DependsOn:
      - DvwaNamespace
    Properties:
      ServiceToken: !GetAtt ApplyManifestFunction.Arn
      Cluster:  !Ref ClusterName
      Manifest:
        - |
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: dvwa
            namespace: dvwa
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: dvwa
            template:
              metadata:
                labels:
                  app: dvwa
              spec:
                containers:
                  - name: dvwa
                    image: vulnerables/web-dvwa:latest
                    ports:
                      - containerPort: 80
        - |
          apiVersion: v1
          kind: Service
          metadata:
            name: dvwa
            namespace: dvwa
          spec:
            type: LoadBalancer
            selector:
              app: dvwa
            ports:
              - port: 80
                targetPort: 80